{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPir0iuKJbHuFocQIHfGwdJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galitneu/auto-eda-tool/blob/main/9498.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csvIVLPsGNfA",
        "outputId": "770366c8-e68d-4153-fc96-85222265eff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "מבצע סינון ראשוני לשמירת נתונים משנת 2008 ואילך...\n",
            "לאחר סינון, נותרו 152203 רשומות בסט האימון.\n",
            "\n",
            "--- שלב הערכה: אימון על נתוני עבר (2008-2010) ובדיקה על 2011 ---\n",
            "ערך הפיספוס (RMSLE) על סט האימות של 2011 הוא: 0.25515\n",
            "\n",
            "==================================================\n",
            "--- שלב סופי: אימון מחדש על כל נתוני האימון (2008-2011) ---\n",
            "אימון המודל הסופי הושלם.\n",
            "\n",
            "==================================================\n",
            "--- יצירת קובץ הגשה על נתוני 2012 באמצעות המודל הסופי ---\n",
            "\n",
            "קובץ ההגשה '/content/drive/MyDrive/KaggleProject/submission.csv' נשמר בהצלחה ב-Google Drive!\n",
            "   SalesID  SalePrice\n",
            "0  1222837    45580.0\n",
            "1  1222839    77070.0\n",
            "2  1222841    35510.0\n",
            "3  1222843    17065.0\n",
            "4  1222845    39230.0\n"
          ]
        }
      ],
      "source": [
        "# --- שלב 0: ייבוא ספריות וחיבור לגוגל דרייב ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# חיבור סביבת העבודה לגוגל דרייב\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- שלב 1: הגדרת נתיבים, טעינת הנתונים וסינון לפי שנה ---\n",
        "DRIVE_PATH = '/content/drive/MyDrive/KaggleProject/'\n",
        "\n",
        "try:\n",
        "    df_train_raw = pd.read_csv(f'{DRIVE_PATH}Train.csv', low_memory=False)\n",
        "    df_valid_raw = pd.read_csv(f'{DRIVE_PATH}Valid.csv', low_memory=False)\n",
        "except FileNotFoundError:\n",
        "    print(f\"שגיאה: ודא שהקבצים 'Train.csv' ו-'Valid.csv' נמצאים בתיקייה: {DRIVE_PATH}\")\n",
        "    exit()\n",
        "\n",
        "# --- סינון הנתונים לשנים 2008 ואילך ---\n",
        "print(\"מבצע סינון ראשוני לשמירת נתונים משנת 2008 ואילך...\")\n",
        "df_train_raw['saledate'] = pd.to_datetime(df_train_raw['saledate'])\n",
        "df_valid_raw['saledate'] = pd.to_datetime(df_valid_raw['saledate'])\n",
        "df_train_raw = df_train_raw[df_train_raw['saledate'].dt.year >= 2008].copy()\n",
        "df_valid_raw = df_valid_raw[df_valid_raw['saledate'].dt.year >= 2008].copy()\n",
        "print(f\"לאחר סינון, נותרו {len(df_train_raw)} רשומות בסט האימון.\")\n",
        "\n",
        "\n",
        "# --- שלב 2: איחוד קבצים לעיבוד אחיד ---\n",
        "\n",
        "# !!! התיקון כאן: יצירת המשתנה train_labels *אחרי* הסינון !!!\n",
        "train_labels = df_train_raw['SalePrice'].copy()\n",
        "df_train_raw = df_train_raw.drop('SalePrice', axis=1)\n",
        "\n",
        "df_train_raw['source'] = 'train'\n",
        "df_valid_raw['source'] = 'valid'\n",
        "df_combined = pd.concat([df_train_raw, df_valid_raw], ignore_index=True, sort=False)\n",
        "\n",
        "# --- שלב 3: הנדסת מאפיינים ---\n",
        "df_combined['saleYear'] = df_combined['saledate'].dt.year\n",
        "df_combined = df_combined.drop('saledate', axis=1)\n",
        "df_combined['machineAge'] = df_combined['saleYear'] - df_combined['YearMade']\n",
        "valid_age_median = df_combined[df_combined['YearMade'] != 1000]['machineAge'].median()\n",
        "df_combined.loc[df_combined['YearMade'] == 1000, 'machineAge'] = valid_age_median\n",
        "df_combined['fiProductClassDesc'] = df_combined['fiProductClassDesc'].fillna('')\n",
        "keywords_to_extract = ['excavator', 'dozer', 'loader', 'crawler', 'wheel', 'track']\n",
        "for keyword in keywords_to_extract:\n",
        "    df_combined[f'is_{keyword}'] = df_combined['fiProductClassDesc'].str.contains(keyword, case=False).astype(int)\n",
        "\n",
        "# --- שלב 4: טיפול בערכים חסרים ---\n",
        "numeric_cols_missing = ['MachineHoursCurrentMeter', 'auctioneerID']\n",
        "for col in numeric_cols_missing:\n",
        "    df_combined[col + '_is_missing'] = df_combined[col].isnull()\n",
        "    median_val = df_combined[col].median()\n",
        "    df_combined[col] = df_combined[col].fillna(median_val)\n",
        "categorical_cols_missing = [col for col in df_combined.columns if pd.api.types.is_object_dtype(df_combined[col]) and df_combined[col].isnull().sum() > 0]\n",
        "for col in categorical_cols_missing:\n",
        "    df_combined[col] = df_combined[col].fillna('missing')\n",
        "\n",
        "# --- שלב 5: המרת עמודות קטגוריאליות למספרים ---\n",
        "source_col = df_combined['source']\n",
        "df_combined = df_combined.drop('source', axis=1)\n",
        "cols_to_drop = []\n",
        "for col_name in df_combined.columns:\n",
        "    if pd.api.types.is_object_dtype(df_combined[col_name]):\n",
        "        num_unique_values = df_combined[col_name].nunique()\n",
        "        if num_unique_values <= 5:\n",
        "            dummies = pd.get_dummies(df_combined[col_name], prefix=col_name)\n",
        "            df_combined = pd.concat([df_combined, dummies], axis=1)\n",
        "            cols_to_drop.append(col_name)\n",
        "        else:\n",
        "            df_combined[col_name] = pd.Categorical(df_combined[col_name]).codes\n",
        "df_combined = df_combined.drop(columns=cols_to_drop)\n",
        "df_combined['source'] = source_col\n",
        "\n",
        "# --- שלב 6: פיצול, אימון מודל והערכה (בשיטת Time-Based) ---\n",
        "df_train_processed = df_combined[df_combined['source'] == 'train'].drop('source', axis=1).copy()\n",
        "df_valid_processed = df_combined[df_combined['source'] == 'valid'].drop('source', axis=1).copy()\n",
        "df_train_processed['SalePrice'] = train_labels.values # .values ensures correct assignment without index alignment issues\n",
        "\n",
        "# 6.1: פיצול מבוסס-זמן לצורך הערכה מקומית\n",
        "val_year = 2011\n",
        "train_time_split = df_train_processed[df_train_processed['saleYear'] <= val_year - 1]\n",
        "val_time_split = df_train_processed[df_train_processed['saleYear'] == val_year]\n",
        "X_train_time = train_time_split.drop('SalePrice', axis=1)\n",
        "y_train_time = train_time_split['SalePrice']\n",
        "X_val_time = val_time_split.drop('SalePrice', axis=1)\n",
        "y_val_time = val_time_split['SalePrice']\n",
        "\n",
        "# 6.2: אימון והערכה על הפיצול המקומי\n",
        "print(\"\\n--- שלב הערכה: אימון על נתוני עבר (2008-2010) ובדיקה על 2011 ---\")\n",
        "model_time_split = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "model_time_split.fit(X_train_time, y_train_time)\n",
        "def rmsle(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
        "val_preds = model_time_split.predict(X_val_time)\n",
        "val_rmsle_score = rmsle(y_val_time, val_preds)\n",
        "print(f\"ערך הפיספוס (RMSLE) על סט האימות של 2011 הוא: {val_rmsle_score:.5f}\")\n",
        "\n",
        "# --- שלב 7: אימון מודל סופי על כל נתוני האימון ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- שלב סופי: אימון מחדש על כל נתוני האימון (2008-2011) ---\")\n",
        "X_full_train = df_train_processed.drop('SalePrice', axis=1)\n",
        "y_full_train = df_train_processed['SalePrice']\n",
        "final_model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "final_model.fit(X_full_train, y_full_train)\n",
        "print(\"אימון המודל הסופי הושלם.\")\n",
        "\n",
        "# --- שלב 8: יצירת קובץ הגשה על נתוני 2012 ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- יצירת קובץ הגשה על נתוני 2012 באמצעות המודל הסופי ---\")\n",
        "train_cols = set(X_full_train.columns)\n",
        "valid_cols = set(df_valid_processed.columns)\n",
        "missing_in_valid = list(train_cols - valid_cols)\n",
        "for c in missing_in_valid:\n",
        "    df_valid_processed[c] = 0\n",
        "df_valid_processed = df_valid_processed[X_full_train.columns]\n",
        "valid_predictions = final_model.predict(df_valid_processed)\n",
        "df_submission = pd.DataFrame({'SalesID': df_valid_raw['SalesID'], 'SalePrice': valid_predictions})\n",
        "submission_filename = f'{DRIVE_PATH}submission.csv'\n",
        "df_submission.to_csv(submission_filename, index=False)\n",
        "print(f\"\\nקובץ ההגשה '{submission_filename}' נשמר בהצלחה ב-Google Drive!\")\n",
        "print(df_submission.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "metadata": {
        "id": "nZGqq23GKSRF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2: הגדרת רשת הפרמטרים לחיפוש אקראי\n",
        "# אלו טווחים רחבים שמהם האלגוריתם ידגום באופן אקראי\n",
        "random_grid = {\n",
        "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n",
        "    'max_features': ['sqrt', 'log2', 1.0], # 1.0 = auto\n",
        "    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)] + [None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# 6.3: יצירה והרצה של החיפוש האקראי\n",
        "print(\"\\n--- מתחיל חיפוש אקראי של היפר-פרמטרים... (זה עשוי לקחת זמן) ---\")\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# n_iter=50 : ננסה 50 צירופים אקראיים\n",
        "# cv=3 : נשתמש באימות צולב עם 3 פולים\n",
        "rs = RandomizedSearchCV(estimator=rf,\n",
        "                          param_distributions=random_grid,\n",
        "                          n_iter=20,\n",
        "                          cv=3,\n",
        "                          verbose=2,\n",
        "                          random_state=42,\n",
        "                          n_jobs=4)\n",
        "\n",
        "# הרצת החיפוש על נתוני האימון מבוססי-הזמן\n",
        "rs.fit(X_train_time, y_train_time)\n",
        "\n",
        "# 6.4: הצגת הפרמטרים הטובים ביותר שנמצאו\n",
        "print(\"\\nהפרמטרים הטובים ביותר שנמצאו בחיפוש האקראי:\")\n",
        "print(rs.best_params_)\n",
        "\n",
        "# 6.5: הערכת המודל עם הפרמטרים הטובים ביותר\n",
        "best_model_random = rs.best_estimator_\n",
        "val_preds = best_model_random.predict(X_val_time)\n",
        "val_rmsle_score = rmsle(y_val_time, val_preds)\n",
        "print(f\"\\nערך הפיספוס (RMSLE) עם המודל המכוונן הוא: {val_rmsle_score:.5f}\")\n",
        "\n",
        "# --- שלב 7: אימון מודל סופי על כל נתוני האימון עם הפרמטרים הטובים ביותר ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- שלב סופי: אימון מחדש על כל נתוני האימון עם הפרמטרים המיטביים ---\")\n",
        "X_full_train = df_train_processed.drop('SalePrice', axis=1)\n",
        "y_full_train = df_train_processed['SalePrice']\n",
        "\n",
        "# יצירת מודל חדש עם הפרמטרים הטובים ביותר שמצאנו\n",
        "final_model = RandomForestRegressor(n_jobs=-1,\n",
        "                                    random_state=42,\n",
        "                                    **rs.best_params_) # <-- שימוש בפרמטרים הטובים ביותר\n",
        "\n",
        "final_model.fit(X_full_train, y_full_train)\n",
        "print(\"אימון המודל הסופי והמכוונן הושלם.\")\n",
        "\n",
        "# --- שלב 8: יצירת קובץ הגשה על נתוני 2012 ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- יצירת קובץ הגשה על נתוני 2012 באמצעות המודל הסופי ---\")\n",
        "train_cols = set(X_full_train.columns)\n",
        "valid_cols = set(df_valid_processed.columns)\n",
        "missing_in_valid = list(train_cols - valid_cols)\n",
        "for c in missing_in_valid:\n",
        "    df_valid_processed[c] = 0\n",
        "df_valid_processed = df_valid_processed[X_full_train.columns]\n",
        "valid_predictions = final_model.predict(df_valid_processed)\n",
        "df_submission = pd.DataFrame({'SalesID': df_valid_raw['SalesID'], 'SalePrice': valid_predictions})\n",
        "submission_filename = f'{DRIVE_PATH}submission.csv'\n",
        "df_submission.to_csv(submission_filename, index=False)\n",
        "print(f\"\\nקובץ ההגשה '{submission_filename}' נשמר בהצלחה ב-Google Drive!\")\n",
        "print(df_submission.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPqoX3W2Kgqf",
        "outputId": "092b9a41-a59e-4619-eb26-5f5f7d65dc4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- מתחיל חיפוש אקראי של היפר-פרמטרים... (זה עשוי לקחת זמן) ---\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ]
        }
      ]
    }
  ]
}