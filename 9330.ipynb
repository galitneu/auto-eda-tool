{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galitneu/auto-eda-tool/blob/main/9330.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csvIVLPsGNfA",
        "outputId": "e460aae5-cdf9-41dc-90a1-2a2d75d587ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "מבצע סינון ראשוני לשמירת נתונים משנת 2008 ואילך...\n",
            "לאחר סינון, נותרו 152203 רשומות בסט האימון.\n",
            "הוסרו עמודות: ['MachineID']\n",
            "\n",
            "--- שלב הערכה: אימון על נתוני עבר (2008-2010) ובדיקה על 2011 ---\n",
            "ערך הפיספוס (RMSLE) על סט האימות של 2011 הוא: 0.24327\n",
            "\n",
            "==================================================\n",
            "--- שלב סופי: אימון מחדש על כל נתוני האימון (2008-2011) ---\n",
            "אימון המודל הסופי הושלם.\n",
            "\n",
            "==================================================\n",
            "--- יצירת קובץ הגשה על נתוני 2012 באמצעות המודל הסופי ---\n",
            "\n",
            "קובץ ההגשה '/content/drive/MyDrive/KaggleProject/submission.csv' נשמר בהצלחה ב-Google Drive!\n",
            "   SalesID     SalePrice\n",
            "0  1222837  51021.783373\n",
            "1  1222839  69508.731384\n",
            "2  1222841  32635.222862\n",
            "3  1222843  16445.745976\n",
            "4  1222845  40607.849021\n"
          ]
        }
      ],
      "source": [
        "# --- שלב 0: ייבוא ספריות וחיבור לגוגל דרייב ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# חיבור סביבת העבודה לגוגל דרייב\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- שלב 1: הגדרת נתיבים, טעינת הנתונים וסינון לפי שנה ---\n",
        "DRIVE_PATH = '/content/drive/MyDrive/KaggleProject/'\n",
        "\n",
        "try:\n",
        "    df_train_raw = pd.read_csv(f'{DRIVE_PATH}Train.csv', low_memory=False)\n",
        "    df_valid_raw = pd.read_csv(f'{DRIVE_PATH}Valid.csv', low_memory=False)\n",
        "except FileNotFoundError:\n",
        "    print(f\"שגיאה: ודא שהקבצים 'Train.csv' ו-'Valid.csv' נמצאים בתיקייה: {DRIVE_PATH}\")\n",
        "    exit()\n",
        "\n",
        "# --- סינון הנתונים לשנים 2008 ואילך ---\n",
        "print(\"מבצע סינון ראשוני לשמירת נתונים משנת 2008 ואילך...\")\n",
        "df_train_raw['saledate'] = pd.to_datetime(df_train_raw['saledate'])\n",
        "df_valid_raw['saledate'] = pd.to_datetime(df_valid_raw['saledate'])\n",
        "df_train_raw = df_train_raw[df_train_raw['saledate'].dt.year >= 2008].copy()\n",
        "df_valid_raw = df_valid_raw[df_valid_raw['saledate'].dt.year >= 2008].copy()\n",
        "print(f\"לאחר סינון, נותרו {len(df_train_raw)} רשומות בסט האימון.\")\n",
        "\n",
        "\n",
        "# --- שלב 2: איחוד קבצים לעיבוד אחיד ---\n",
        "\n",
        "# !!! התיקון כאן: יצירת המשתנה train_labels *אחרי* הסינון !!!\n",
        "train_labels = df_train_raw['SalePrice'].copy()\n",
        "df_train_raw = df_train_raw.drop('SalePrice', axis=1)\n",
        "\n",
        "df_train_raw['source'] = 'train'\n",
        "df_valid_raw['source'] = 'valid'\n",
        "df_combined = pd.concat([df_train_raw, df_valid_raw], ignore_index=True, sort=False)\n",
        "\n",
        "\n",
        "# הסרת עמודות ID שאינן רלוונטיות למודל\n",
        "columns_to_remove = ['MachineID']\n",
        "existing_columns_to_remove = [col for col in columns_to_remove if col in df_combined.columns]\n",
        "if existing_columns_to_remove:\n",
        "    df_combined = df_combined.drop(existing_columns_to_remove, axis=1)\n",
        "    print(f\"הוסרו עמודות: {existing_columns_to_remove}\")\n",
        "\n",
        "\n",
        "# --- שלב 3: הנדסת מאפיינים ---\n",
        "df_combined['saleYear'] = df_combined['saledate'].dt.year\n",
        "df_combined = df_combined.drop('saledate', axis=1)\n",
        "df_combined['machineAge'] = df_combined['saleYear'] - df_combined['YearMade']\n",
        "valid_age_median = df_combined[df_combined['YearMade'] != 1000]['machineAge'].median()\n",
        "df_combined.loc[df_combined['YearMade'] == 1000, 'machineAge'] = valid_age_median\n",
        "df_combined['fiProductClassDesc'] = df_combined['fiProductClassDesc'].fillna('')\n",
        "keywords_to_extract = ['excavator', 'dozer', 'loader', 'crawler', 'wheel', 'track']\n",
        "for keyword in keywords_to_extract:\n",
        "    df_combined[f'is_{keyword}'] = df_combined['fiProductClassDesc'].str.contains(keyword, case=False).astype(int)\n",
        "\n",
        "# --- שלב 4: טיפול בערכים חסרים ---\n",
        "numeric_cols_missing = ['MachineHoursCurrentMeter', 'auctioneerID']\n",
        "for col in numeric_cols_missing:\n",
        "    df_combined[col + '_is_missing'] = df_combined[col].isnull()\n",
        "    median_val = df_combined[col].median()\n",
        "    df_combined[col] = df_combined[col].fillna(median_val)\n",
        "categorical_cols_missing = [col for col in df_combined.columns if pd.api.types.is_object_dtype(df_combined[col]) and df_combined[col].isnull().sum() > 0]\n",
        "for col in categorical_cols_missing:\n",
        "    df_combined[col] = df_combined[col].fillna('missing')\n",
        "\n",
        "# --- שלב 5: המרת עמודות קטגוריאליות למספרים ---\n",
        "source_col = df_combined['source']\n",
        "df_combined = df_combined.drop('source', axis=1)\n",
        "cols_to_drop = []\n",
        "for col_name in df_combined.columns:\n",
        "    if pd.api.types.is_object_dtype(df_combined[col_name]):\n",
        "        num_unique_values = df_combined[col_name].nunique()\n",
        "        if num_unique_values <= 5:\n",
        "            dummies = pd.get_dummies(df_combined[col_name], prefix=col_name)\n",
        "            df_combined = pd.concat([df_combined, dummies], axis=1)\n",
        "            cols_to_drop.append(col_name)\n",
        "        else:\n",
        "            df_combined[col_name] = pd.Categorical(df_combined[col_name]).codes\n",
        "df_combined = df_combined.drop(columns=cols_to_drop)\n",
        "df_combined['source'] = source_col\n",
        "\n",
        "# --- שלב 6: פיצול, אימון מודל והערכה (בשיטת Time-Based) ---\n",
        "df_train_processed = df_combined[df_combined['source'] == 'train'].drop('source', axis=1).copy()\n",
        "df_valid_processed = df_combined[df_combined['source'] == 'valid'].drop('source', axis=1).copy()\n",
        "df_train_processed['SalePrice'] = train_labels.values # .values ensures correct assignment without index alignment issues\n",
        "\n",
        "# 6.1: פיצול מבוסס-זמן לצורך הערכה מקומית\n",
        "val_year = 2011\n",
        "train_time_split = df_train_processed[df_train_processed['saleYear'] <= val_year - 1]\n",
        "val_time_split = df_train_processed[df_train_processed['saleYear'] == val_year]\n",
        "X_train_time = train_time_split.drop('SalePrice', axis=1)\n",
        "y_train_time = train_time_split['SalePrice']\n",
        "X_val_time = val_time_split.drop('SalePrice', axis=1)\n",
        "y_val_time = val_time_split['SalePrice']\n",
        "\n",
        "# 6.2: אימון והערכה על הפיצול המקומי\n",
        "print(\"\\n--- שלב הערכה: אימון על נתוני עבר (2008-2010) ובדיקה על 2011 ---\")\n",
        "model_time_split = RandomForestRegressor(n_jobs=-1, random_state=42,n_estimators= 150, min_samples_split= 5, min_samples_leaf= 4, max_features= 0.5, max_depth= 30)\n",
        "model_time_split.fit(X_train_time, y_train_time)\n",
        "def rmsle(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
        "val_preds = model_time_split.predict(X_val_time)\n",
        "val_rmsle_score = rmsle(y_val_time, val_preds)\n",
        "print(f\"ערך הפיספוס (RMSLE) על סט האימות של 2011 הוא: {val_rmsle_score:.5f}\")\n",
        "\n",
        "# --- שלב 7: אימון מודל סופי על כל נתוני האימון ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- שלב סופי: אימון מחדש על כל נתוני האימון (2008-2011) ---\")\n",
        "X_full_train = df_train_processed.drop('SalePrice', axis=1)\n",
        "y_full_train = df_train_processed['SalePrice']\n",
        "final_model = RandomForestRegressor(n_jobs=-1, random_state=42,n_estimators= 150, min_samples_split= 5, min_samples_leaf= 4, max_features= 0.5, max_depth= 30)\n",
        "final_model.fit(X_full_train, y_full_train)\n",
        "print(\"אימון המודל הסופי הושלם.\")\n",
        "\n",
        "# --- שלב 8: יצירת קובץ הגשה על נתוני 2012 ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- יצירת קובץ הגשה על נתוני 2012 באמצעות המודל הסופי ---\")\n",
        "train_cols = set(X_full_train.columns)\n",
        "valid_cols = set(df_valid_processed.columns)\n",
        "missing_in_valid = list(train_cols - valid_cols)\n",
        "for c in missing_in_valid:\n",
        "    df_valid_processed[c] = 0\n",
        "df_valid_processed = df_valid_processed[X_full_train.columns]\n",
        "valid_predictions = final_model.predict(df_valid_processed)\n",
        "df_submission = pd.DataFrame({'SalesID': df_valid_raw['SalesID'], 'SalePrice': valid_predictions})\n",
        "submission_filename = f'{DRIVE_PATH}submission.csv'\n",
        "df_submission.to_csv(submission_filename, index=False)\n",
        "print(f\"\\nקובץ ההגשה '{submission_filename}' נשמר בהצלחה ב-Google Drive!\")\n",
        "print(df_submission.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZGqq23GKSRF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPqoX3W2Kgqf",
        "outputId": "a1d0bf65-a3ab-4816-d24c-0513ed1c8eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- מתחיל חיפוש אקראי של היפר-פרמטרים... (זה עשוי לקחת זמן) ---\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   4.7s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   4.4s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.2s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   4.8s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.6s\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time=   6.6s\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time=   8.7s\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time=   6.6s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  31.1s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  29.9s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  29.1s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  23.6s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  25.6s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  24.6s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.4s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.3s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.3s\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.3s\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   4.8s\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   5.0s\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  12.0s\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  14.8s\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  12.9s\n",
            "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=4, min_samples_split=5, n_estimators=150; total time= 1.2min\n",
            "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=4, min_samples_split=5, n_estimators=150; total time= 1.3min\n",
            "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=4, min_samples_split=5, n_estimators=150; total time= 1.3min\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time= 1.2min\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time= 1.3min\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time= 1.2min\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time= 1.4min\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time= 1.5min\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time= 1.5min\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   8.5s\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   9.6s\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  10.0s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   6.1s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   8.1s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   6.2s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  26.1s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  25.1s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  23.8s\n",
            "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=  18.7s\n",
            "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=  18.9s\n",
            "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=  20.1s\n",
            "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=150; total time=  16.6s\n",
            "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=150; total time=  17.1s\n",
            "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=150; total time=  16.7s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=150; total time=  10.7s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=150; total time=  11.1s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=150; total time=  10.0s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  50.5s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  49.2s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  49.9s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time= 1.5min\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time= 1.5min\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time= 1.6min\n",
            "\n",
            "הפרמטרים הטובים ביותר שנמצאו בחיפוש האקראי:\n",
            "{'n_estimators': 150, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 0.5, 'max_depth': 30}\n",
            "\n",
            "ערך הפיספוס (RMSLE) עם המודל המכוונן הוא: 0.24463\n",
            "\n",
            "==================================================\n",
            "--- שלב סופי: אימון מחדש על כל נתוני האימון עם הפרמטרים המיטביים ---\n",
            "אימון המודל הסופי והמכוונן הושלם.\n"
          ]
        }
      ],
      "source": [
        "# 6.3: יצירה והרצה של החיפוש האקראי\n",
        "print(\"\\n--- מתחיל חיפוש אקראי של היפר-פרמטרים... (זה עשוי לקחת זמן) ---\")\n",
        "\n",
        "# ... (הגדרת param_dist ו-rs נשארת זהה)\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_features': [0.5, 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "rs_base_model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=rs_base_model,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=20,\n",
        "                                   cv=3,\n",
        "                                   verbose=2,\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=1)\n",
        "\n",
        "# התיקון כאן: הרצת החיפוש על האובייקט הנכון\n",
        "random_search.fit(X_train_time, y_train_time)\n",
        "\n",
        "\n",
        "# 6.4: הצגת הפרמטרים הטובים ביותר שנמצאו\n",
        "print(\"\\nהפרמטרים הטובים ביותר שנמצאו בחיפוש האקראי:\")\n",
        "# שימוש באובייקט הנכון כדי לקבל את התוצאות\n",
        "best_model_random = random_search.best_estimator_\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# 6.5: הערכת המודל עם הפרמטרים הטובים ביותר\n",
        "val_preds = best_model_random.predict(X_val_time)\n",
        "val_rmsle_score = rmsle(y_val_time, val_preds)\n",
        "print(f\"\\nערך הפיספוס (RMSLE) עם המודל המכוונן הוא: {val_rmsle_score:.5f}\")\n",
        "\n",
        "# --- שלב 7: אימון מודל סופי עם הפרמטרים הטובים ביותר ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- שלב סופי: אימון מחדש על כל נתוני האימון עם הפרמטרים המיטביים ---\")\n",
        "X_full_train = df_train_processed.drop('SalePrice', axis=1)\n",
        "y_full_train = df_train_processed['SalePrice']\n",
        "\n",
        "# התיקון כאן: שימוש בפרמטרים מהאובייקט הנכון\n",
        "final_model = RandomForestRegressor(n_jobs=-1,\n",
        "                                    random_state=42,\n",
        "                                    **random_search.best_params_)\n",
        "\n",
        "final_model.fit(X_full_train, y_full_train)\n",
        "print(\"אימון המודל הסופי והמכוונן הושלם.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*{'n_estimators': 150, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 0.5, 'max_depth': 30}\n"
      ],
      "metadata": {
        "id": "22EaixgRQMF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- שלב 8: יצירת קובץ הגשה על נתוני 2012 ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- יצירת קובץ הגשה על נתוני 2012 באמצעות המודל הסופי ---\")\n",
        "train_cols = set(X_full_train.columns)\n",
        "valid_cols = set(df_valid_processed.columns)\n",
        "missing_in_valid = list(train_cols - valid_cols)\n",
        "for c in missing_in_valid:\n",
        "    df_valid_processed[c] = 0\n",
        "df_valid_processed = df_valid_processed[X_full_train.columns]\n",
        "valid_predictions = final_model.predict(df_valid_processed)\n",
        "df_submission = pd.DataFrame({'SalesID': df_valid_raw['SalesID'], 'SalePrice': valid_predictions})\n",
        "submission_filename = f'{DRIVE_PATH}submission.csv'\n",
        "df_submission.to_csv(submission_filename, index=False)\n",
        "print(f\"\\nקובץ ההגשה '{submission_filename}' נשמר בהצלחה ב-Google Drive!\")\n",
        "print(df_submission.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMbuYNN5Q1Nb",
        "outputId": "6c41e36b-11f7-4b2e-a05b-9a5bb62707da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "--- יצירת קובץ הגשה על נתוני 2012 באמצעות המודל הסופי ---\n",
            "\n",
            "קובץ ההגשה '/content/drive/MyDrive/KaggleProject/submission.csv' נשמר בהצלחה ב-Google Drive!\n",
            "   SalesID     SalePrice\n",
            "0  1222837  45631.943917\n",
            "1  1222839  71520.674511\n",
            "2  1222841  34326.652104\n",
            "3  1222843  16260.024516\n",
            "4  1222845  40855.074722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "import shap\n",
        "from sklearn.inspection import permutation_importance\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSUwtmz5UFl1",
        "outputId": "31ba42b0-b118-4225-88e9-b7b3eacbc975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.48.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from shap) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.14.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- שלב 8: ניתוח חשיבות מאפיינים ---\n",
        "print(\"\\n--- ניתוח חשיבות מאפיינים ---\")\n",
        "\n",
        "# Feature Importance רגיל\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train_time.columns,\n",
        "    'importance': final_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 15 מאפיינים חשובים:\")\n",
        "print(feature_importance.head(15))\n",
        "\n",
        "# Permutation Importance\n",
        "print(\"\\nמחשב Permutation Importance...\")\n",
        "perm_importance = permutation_importance(\n",
        "    final_model, X_val_time, y_val_time,\n",
        "    scoring='neg_mean_squared_log_error',\n",
        "    n_repeats=3, random_state=42, n_jobs=1  # מספר חזרות קטן יותר וללא מקבילות\n",
        ")\n",
        "\n",
        "perm_importance_df = pd.DataFrame({\n",
        "    'feature': X_train_time.columns,\n",
        "    'importance_mean': perm_importance.importances_mean,\n",
        "    'importance_std': perm_importance.importances_std\n",
        "}).sort_values('importance_mean', ascending=False)\n",
        "\n",
        "print(\"Top 15 מאפיינים לפי Permutation Importance:\")\n",
        "print(perm_importance_df.head(15))\n",
        "\n",
        "# --- שלב 9: SHAP Analysis ---\n",
        "print(\"\\n--- ניתוח SHAP ---\")\n",
        "\n",
        "try:\n",
        "    # דגימה קטנה לניתוח SHAP (כדי לחסוך זמן)\n",
        "    sample_size = min(500, len(X_train_time))  # הקטנת הדגימה\n",
        "    X_sample = X_train_time.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    print(\"יוצר SHAP explainer...\")\n",
        "    # יצירת מודל חדש ללא n_jobs כדי למנוע בעיות pickling\n",
        "    rf_for_shap = RandomForestRegressor(\n",
        "        n_estimators=getattr(final_model, 'n_estimators', 100),\n",
        "        max_depth=getattr(final_model, 'max_depth', None),\n",
        "        min_samples_split=getattr(final_model, 'min_samples_split', 2),\n",
        "        min_samples_leaf=getattr(final_model, 'min_samples_leaf', 1),\n",
        "        max_features=getattr(final_model, 'max_features', 'sqrt'),\n",
        "        random_state=42,\n",
        "        n_jobs=1  # ללא מקבילות ל-SHAP\n",
        "    )\n",
        "    rf_for_shap.fit(X_train_time, y_train_time)\n",
        "\n",
        "    explainer = shap.TreeExplainer(rf_for_shap)\n",
        "    print(\"מחשב SHAP values...\")\n",
        "    shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "    # SHAP Summary Plot\n",
        "    print(\"יוצר SHAP summary plot...\")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.summary_plot(shap_values, X_sample, max_display=20, show=False)\n",
        "    plt.title('SHAP Summary Plot - Top 20 Features')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{DRIVE_PATH}shap_summary_plot.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # SHAP Feature Importance\n",
        "    shap_importance = pd.DataFrame({\n",
        "        'feature': X_sample.columns,\n",
        "        'shap_importance': np.abs(shap_values).mean(0)\n",
        "    }).sort_values('shap_importance', ascending=False)\n",
        "\n",
        "    print(\"Top 15 מאפיינים לפי SHAP:\")\n",
        "    print(shap_importance.head(15))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"שגיאה בניתוח SHAP: {e}\")\n",
        "    print(\"ממשיך בלי SHAP analysis...\")\n",
        "    # יצירת shap_importance ריק כדי שהקוד ימשיך לעבוד\n",
        "    shap_importance = pd.DataFrame({\n",
        "        'feature': X_train_time.columns,\n",
        "        'shap_importance': np.zeros(len(X_train_time.columns))\n",
        "    })\n",
        "\n",
        "# --- שלב 10: הסרת מאפיינים לא חשובים ---\n",
        "print(\"\\n--- מסנן מאפיינים לא חשובים ---\")\n",
        "\n",
        "# בחירת מאפיינים על בסיס מספר מקורות חשיבות\n",
        "top_features_rf = set(feature_importance.head(50)['feature'])\n",
        "top_features_perm = set(perm_importance_df.head(50)['feature'])\n",
        "\n",
        "# אם SHAP עבד, נשתמש בו גם\n",
        "if 'shap_importance' in locals() and len(shap_importance) > 0 and shap_importance['shap_importance'].sum() > 0:\n",
        "    top_features_shap = set(shap_importance.head(50)['feature'])\n",
        "    # מאפיינים שמופיעים בלפחות 2 מתוך 3 הרשימות\n",
        "    important_features = (top_features_rf & top_features_perm) | \\\n",
        "                        (top_features_rf & top_features_shap) | \\\n",
        "                        (top_features_perm & top_features_shap)\n",
        "    print(f\"משתמש ב-3 שיטות לבחירת מאפיינים\")\n",
        "else:\n",
        "    # אם SHAP לא עבד, נשתמש רק ב-2 השיטות\n",
        "    important_features = top_features_rf & top_features_perm\n",
        "    if len(important_features) < 20:  # אם יש מעט מאפיינים משותפים\n",
        "        important_features = top_features_rf | top_features_perm\n",
        "    print(f\"משתמש ב-2 שיטות לבחירת מאפיינים (ללא SHAP)\")\n",
        "\n",
        "print(f\"נבחרו {len(important_features)} מאפיינים חשובים מתוך {len(X_train_time.columns)}\")\n",
        "\n",
        "# אימון מחדש עם מאפיינים נבחרים\n",
        "X_train_time_selected = X_train_time[list(important_features)]\n",
        "X_val_time_selected = X_val_time[list(important_features)]\n",
        "\n",
        "final_rf_selected = RandomForestRegressor(\n",
        "    n_estimators=getattr(final_model, 'n_estimators', 100),\n",
        "    max_depth=getattr(final_model, 'max_depth', None),\n",
        "    min_samples_split=getattr(final_model, 'min_samples_split', 2),\n",
        "    min_samples_leaf=getattr(final_model, 'min_samples_leaf', 1),\n",
        "    max_features=getattr(final_model, 'max_features', 'sqrt'),\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "final_rf_selected.fit(X_train_time_selected, y_train_time)\n",
        "\n",
        "val_preds_selected = final_rf_selected.predict(X_val_time_selected)\n",
        "val_rmsle_selected = rmsle(y_val_time, val_preds_selected)\n",
        "print(f\"RMSLE לאחר בחירת מאפיינים: {val_rmsle_selected:.5f}\")\n",
        "\n",
        "# --- שלב 11: אימון מודל סופי ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- אימון מודל סופי ---\")\n",
        "\n",
        "X_full_train = df_train_processed.drop('SalePrice', axis=1)\n",
        "y_full_train = df_train_processed['SalePrice']\n",
        "\n",
        "# בחירת המודל הטוב יותר\n",
        "if val_rmsle_selected < val_rmsle_current:\n",
        "    print(f\"משתמש במודל עם מאפיינים נבחרים (RMSLE: {val_rmsle_selected:.5f})\")\n",
        "    X_full_train_final = X_full_train[list(important_features)]\n",
        "    best_model = RandomForestRegressor(\n",
        "        n_estimators=getattr(final_model, 'n_estimators', 100),\n",
        "        max_depth=getattr(final_model, 'max_depth', None),\n",
        "        min_samples_split=getattr(final_model, 'min_samples_split', 2),\n",
        "        min_samples_leaf=getattr(final_model, 'min_samples_leaf', 1),\n",
        "        max_features=getattr(final_model, 'max_features', 'sqrt'),\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    selected_features = list(important_features)\n",
        "    best_rmsle = val_rmsle_selected\n",
        "else:\n",
        "    print(f\"משתמש במודל עם כל המאפיינים (RMSLE: {val_rmsle_current:.5f})\")\n",
        "    X_full_train_final = X_full_train\n",
        "    best_model = final_model\n",
        "    selected_features = list(X_full_train.columns)\n",
        "    best_rmsle = val_rmsle_current\n",
        "\n",
        "best_model.fit(X_full_train_final, y_full_train)\n",
        "print(\"אימון המודל הסופי הושלם.\")\n",
        "\n",
        "# --- שלב 12: יצירת קובץ הגשה ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- יצירת קובץ הגשה ---\")\n",
        "\n",
        "# הכנת נתוני validation\n",
        "train_cols = set(selected_features)\n",
        "valid_cols = set(df_valid_processed.columns)\n",
        "missing_in_valid = list(train_cols - valid_cols)\n",
        "\n",
        "for c in missing_in_valid:\n",
        "    df_valid_processed[c] = 0\n",
        "\n",
        "df_valid_final = df_valid_processed[selected_features]\n",
        "valid_predictions = best_model.predict(df_valid_final)\n",
        "\n",
        "# יצירת קובץ הגשה\n",
        "df_submission = pd.DataFrame({\n",
        "    'SalesID': df_valid_raw['SalesID'],\n",
        "    'SalePrice': valid_predictions\n",
        "})\n",
        "\n",
        "submission_filename = f'{DRIVE_PATH}submission_improved.csv'\n",
        "df_submission.to_csv(submission_filename, index=False)\n",
        "print(f\"\\nקובץ ההגשה '{submission_filename}' נשמר בהצלחה!\")\n",
        "print(f\"דוגמה מהקובץ:\")\n",
        "print(df_submission.head())\n",
        "\n",
        "# --- שלב 13: סיכום וויזואליזציות נוספות ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- סיכום ---\")\n",
        "print(f\"RMSLE בסיסי: {val_rmsle_basic:.5f}\")\n",
        "print(f\"RMSLE סופי: {best_rmsle:.5f}\")\n",
        "print(f\"שיפור: {((val_rmsle_basic - best_rmsle) / val_rmsle_basic * 100):.2f}%\")\n",
        "print(f\"מספר מאפיינים במודל הסופי: {len(selected_features)}\")\n",
        "\n",
        "# Plot comparison של חשיבויות\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Subplot 1: Feature Importance\n",
        "plt.subplot(2, 2, 1)\n",
        "top_features = feature_importance.head(15)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Random Forest Feature Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Subplot 2: Permutation Importance\n",
        "plt.subplot(2, 2, 2)\n",
        "top_perm = perm_importance_df.head(15)\n",
        "plt.barh(range(len(top_perm)), top_perm['importance_mean'])\n",
        "plt.yticks(range(len(top_perm)), top_perm['feature'])\n",
        "plt.xlabel('Permutation Importance')\n",
        "plt.title('Permutation Feature Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Subplot 3: SHAP Importance (אם זמין)\n",
        "if 'shap_importance' in locals() and len(shap_importance) > 0 and shap_importance['shap_importance'].sum() > 0:\n",
        "    plt.subplot(2, 2, 3)\n",
        "    top_shap = shap_importance.head(15)\n",
        "    plt.barh(range(len(top_shap)), top_shap['shap_importance'])\n",
        "    plt.yticks(range(len(top_shap)), top_shap['feature'])\n",
        "    plt.xlabel('SHAP Importance')\n",
        "    plt.title('SHAP Feature Importance')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    # Subplot 4: Prediction vs Actual\n",
        "    plt.subplot(2, 2, 4)\n",
        "else:\n",
        "    # אם אין SHAP, נשתמש בכל השטח לגרף חיזוי מול אמיתי\n",
        "    plt.subplot(2, 1, 2)  # שינוי לפורמט 2x1\n",
        "\n",
        "# גרף חיזוי מול אמיתי\n",
        "if val_rmsle_selected < val_rmsle_current:\n",
        "    preds_to_plot = val_preds_selected\n",
        "else:\n",
        "    preds_to_plot = val_preds_current\n",
        "\n",
        "plt.scatter(y_val_time, preds_to_plot, alpha=0.5)\n",
        "plt.plot([y_val_time.min(), y_val_time.max()], [y_val_time.min(), y_val_time.max()], 'r--', lw=2)\n",
        "plt.xlabel('מחיר אמיתי')\n",
        "plt.ylabel('מחיר חזוי')\n",
        "plt.title(f'חזוי מול אמיתי (RMSLE: {best_rmsle:.5f})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{DRIVE_PATH}feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nהתמונות נשמרו ב-{DRIVE_PATH}\")\n",
        "print(\"שיפורים נוספים לשקול:\")\n",
        "print(\"1. Ensemble של מספר מודלים (XGBoost, LightGBM)\")\n",
        "print(\"2. Cross-validation מתקדם יותר\")\n",
        "print(\"3. Target encoding עבור קטגוריות עם cardinality גבוה\")\n",
        "print(\"4. הנדסת מאפיינים נוספת על בסיס domain knowledge\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvw40sxrTvlu",
        "outputId": "b635ff23-3e70-4b94-bf3f-f47bf73e68bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ניתוח חשיבות מאפיינים ---\n",
            "Top 15 מאפיינים חשובים:\n",
            "                   feature  importance\n",
            "12             ProductSize    0.178980\n",
            "5                 YearMade    0.144784\n",
            "17               Enclosure    0.099059\n",
            "27              machineAge    0.062746\n",
            "13      fiProductClassDesc    0.061466\n",
            "7              fiModelDesc    0.057976\n",
            "2                  ModelID    0.046791\n",
            "8              fiBaseModel    0.046735\n",
            "9          fiSecondaryDesc    0.033237\n",
            "1                MachineID    0.031314\n",
            "94  Coupler_System_missing    0.023957\n",
            "11       fiModelDescriptor    0.019733\n",
            "21               Tire_Size    0.015490\n",
            "26                saleYear    0.014471\n",
            "97  Grouser_Tracks_missing    0.013393\n",
            "\n",
            "מחשב Permutation Importance...\n",
            "Top 15 מאפיינים לפי Permutation Importance:\n",
            "                     feature  importance_mean  importance_std\n",
            "12               ProductSize         0.235143        0.001444\n",
            "5                   YearMade         0.158028        0.000232\n",
            "13        fiProductClassDesc         0.059297        0.000135\n",
            "27                machineAge         0.042202        0.000536\n",
            "94    Coupler_System_missing         0.037859        0.000162\n",
            "17                 Enclosure         0.033190        0.000373\n",
            "7                fiModelDesc         0.024464        0.000115\n",
            "21                 Tire_Size         0.022852        0.000580\n",
            "2                    ModelID         0.021624        0.000217\n",
            "9            fiSecondaryDesc         0.020372        0.000440\n",
            "8                fiBaseModel         0.017908        0.000069\n",
            "11         fiModelDescriptor         0.012864        0.000399\n",
            "101  Hydraulics_Flow_missing         0.009277        0.000045\n",
            "1                  MachineID         0.009012        0.000122\n",
            "97    Grouser_Tracks_missing         0.008586        0.000052\n",
            "\n",
            "--- ניתוח SHAP ---\n",
            "יוצר SHAP explainer...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3+0/HeR0c00K5rZrCesav",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}