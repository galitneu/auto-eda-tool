{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galitneu/auto-eda-tool/blob/main/9300.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csvIVLPsGNfA",
        "outputId": "89036681-1539-4f7f-a9de-f8284a676065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "מבצע סינון ראשוני לשמירת נתונים משנת 2008 ואילך...\n",
            "לאחר סינון, נותרו 152203 רשומות בסט האימון.\n",
            "\n",
            "--- שלב הערכה: אימון על נתוני עבר (2008-2010) ובדיקה על 2011 ---\n",
            "ערך הפיספוס (RMSLE) על סט האימות של 2011 הוא: 0.25515\n",
            "\n",
            "==================================================\n",
            "--- שלב סופי: אימון מחדש על כל נתוני האימון (2008-2011) ---\n",
            "אימון המודל הסופי הושלם.\n",
            "\n",
            "==================================================\n",
            "--- יצירת קובץ הגשה על נתוני 2012 באמצעות המודל הסופי ---\n",
            "\n",
            "קובץ ההגשה '/content/drive/MyDrive/KaggleProject/submission.csv' נשמר בהצלחה ב-Google Drive!\n",
            "   SalesID  SalePrice\n",
            "0  1222837    45580.0\n",
            "1  1222839    77070.0\n",
            "2  1222841    35510.0\n",
            "3  1222843    17065.0\n",
            "4  1222845    39230.0\n"
          ]
        }
      ],
      "source": [
        "# --- שלב 0: ייבוא ספריות וחיבור לגוגל דרייב ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# חיבור סביבת העבודה לגוגל דרייב\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- שלב 1: הגדרת נתיבים, טעינת הנתונים וסינון לפי שנה ---\n",
        "DRIVE_PATH = '/content/drive/MyDrive/KaggleProject/'\n",
        "\n",
        "try:\n",
        "    df_train_raw = pd.read_csv(f'{DRIVE_PATH}Train.csv', low_memory=False)\n",
        "    df_valid_raw = pd.read_csv(f'{DRIVE_PATH}Valid.csv', low_memory=False)\n",
        "except FileNotFoundError:\n",
        "    print(f\"שגיאה: ודא שהקבצים 'Train.csv' ו-'Valid.csv' נמצאים בתיקייה: {DRIVE_PATH}\")\n",
        "    exit()\n",
        "\n",
        "# --- סינון הנתונים לשנים 2008 ואילך ---\n",
        "print(\"מבצע סינון ראשוני לשמירת נתונים משנת 2008 ואילך...\")\n",
        "df_train_raw['saledate'] = pd.to_datetime(df_train_raw['saledate'])\n",
        "df_valid_raw['saledate'] = pd.to_datetime(df_valid_raw['saledate'])\n",
        "df_train_raw = df_train_raw[df_train_raw['saledate'].dt.year >= 2008].copy()\n",
        "df_valid_raw = df_valid_raw[df_valid_raw['saledate'].dt.year >= 2008].copy()\n",
        "print(f\"לאחר סינון, נותרו {len(df_train_raw)} רשומות בסט האימון.\")\n",
        "\n",
        "\n",
        "# --- שלב 2: איחוד קבצים לעיבוד אחיד ---\n",
        "\n",
        "# !!! התיקון כאן: יצירת המשתנה train_labels *אחרי* הסינון !!!\n",
        "train_labels = df_train_raw['SalePrice'].copy()\n",
        "df_train_raw = df_train_raw.drop('SalePrice', axis=1)\n",
        "\n",
        "df_train_raw['source'] = 'train'\n",
        "df_valid_raw['source'] = 'valid'\n",
        "df_combined = pd.concat([df_train_raw, df_valid_raw], ignore_index=True, sort=False)\n",
        "\n",
        "# --- שלב 3: הנדסת מאפיינים ---\n",
        "df_combined['saleYear'] = df_combined['saledate'].dt.year\n",
        "df_combined = df_combined.drop('saledate', axis=1)\n",
        "df_combined['machineAge'] = df_combined['saleYear'] - df_combined['YearMade']\n",
        "valid_age_median = df_combined[df_combined['YearMade'] != 1000]['machineAge'].median()\n",
        "df_combined.loc[df_combined['YearMade'] == 1000, 'machineAge'] = valid_age_median\n",
        "df_combined['fiProductClassDesc'] = df_combined['fiProductClassDesc'].fillna('')\n",
        "keywords_to_extract = ['excavator', 'dozer', 'loader', 'crawler', 'wheel', 'track']\n",
        "for keyword in keywords_to_extract:\n",
        "    df_combined[f'is_{keyword}'] = df_combined['fiProductClassDesc'].str.contains(keyword, case=False).astype(int)\n",
        "\n",
        "# --- שלב 4: טיפול בערכים חסרים ---\n",
        "numeric_cols_missing = ['MachineHoursCurrentMeter', 'auctioneerID']\n",
        "for col in numeric_cols_missing:\n",
        "    df_combined[col + '_is_missing'] = df_combined[col].isnull()\n",
        "    median_val = df_combined[col].median()\n",
        "    df_combined[col] = df_combined[col].fillna(median_val)\n",
        "categorical_cols_missing = [col for col in df_combined.columns if pd.api.types.is_object_dtype(df_combined[col]) and df_combined[col].isnull().sum() > 0]\n",
        "for col in categorical_cols_missing:\n",
        "    df_combined[col] = df_combined[col].fillna('missing')\n",
        "\n",
        "# --- שלב 5: המרת עמודות קטגוריאליות למספרים ---\n",
        "source_col = df_combined['source']\n",
        "df_combined = df_combined.drop('source', axis=1)\n",
        "cols_to_drop = []\n",
        "for col_name in df_combined.columns:\n",
        "    if pd.api.types.is_object_dtype(df_combined[col_name]):\n",
        "        num_unique_values = df_combined[col_name].nunique()\n",
        "        if num_unique_values <= 5:\n",
        "            dummies = pd.get_dummies(df_combined[col_name], prefix=col_name)\n",
        "            df_combined = pd.concat([df_combined, dummies], axis=1)\n",
        "            cols_to_drop.append(col_name)\n",
        "        else:\n",
        "            df_combined[col_name] = pd.Categorical(df_combined[col_name]).codes\n",
        "df_combined = df_combined.drop(columns=cols_to_drop)\n",
        "df_combined['source'] = source_col\n",
        "\n",
        "# --- שלב 6: פיצול, אימון מודל והערכה (בשיטת Time-Based) ---\n",
        "df_train_processed = df_combined[df_combined['source'] == 'train'].drop('source', axis=1).copy()\n",
        "df_valid_processed = df_combined[df_combined['source'] == 'valid'].drop('source', axis=1).copy()\n",
        "df_train_processed['SalePrice'] = train_labels.values # .values ensures correct assignment without index alignment issues\n",
        "\n",
        "# 6.1: פיצול מבוסס-זמן לצורך הערכה מקומית\n",
        "val_year = 2011\n",
        "train_time_split = df_train_processed[df_train_processed['saleYear'] <= val_year - 1]\n",
        "val_time_split = df_train_processed[df_train_processed['saleYear'] == val_year]\n",
        "X_train_time = train_time_split.drop('SalePrice', axis=1)\n",
        "y_train_time = train_time_split['SalePrice']\n",
        "X_val_time = val_time_split.drop('SalePrice', axis=1)\n",
        "y_val_time = val_time_split['SalePrice']\n",
        "\n",
        "# 6.2: אימון והערכה על הפיצול המקומי\n",
        "print(\"\\n--- שלב הערכה: אימון על נתוני עבר (2008-2010) ובדיקה על 2011 ---\")\n",
        "model_time_split = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "model_time_split.fit(X_train_time, y_train_time)\n",
        "def rmsle(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
        "val_preds = model_time_split.predict(X_val_time)\n",
        "val_rmsle_score = rmsle(y_val_time, val_preds)\n",
        "print(f\"ערך הפיספוס (RMSLE) על סט האימות של 2011 הוא: {val_rmsle_score:.5f}\")\n",
        "\n",
        "# --- שלב 7: אימון מודל סופי על כל נתוני האימון ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- שלב סופי: אימון מחדש על כל נתוני האימון (2008-2011) ---\")\n",
        "X_full_train = df_train_processed.drop('SalePrice', axis=1)\n",
        "y_full_train = df_train_processed['SalePrice']\n",
        "final_model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "final_model.fit(X_full_train, y_full_train)\n",
        "print(\"אימון המודל הסופי הושלם.\")\n",
        "\n",
        "# --- שלב 8: יצירת קובץ הגשה על נתוני 2012 ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- יצירת קובץ הגשה על נתוני 2012 באמצעות המודל הסופי ---\")\n",
        "train_cols = set(X_full_train.columns)\n",
        "valid_cols = set(df_valid_processed.columns)\n",
        "missing_in_valid = list(train_cols - valid_cols)\n",
        "for c in missing_in_valid:\n",
        "    df_valid_processed[c] = 0\n",
        "df_valid_processed = df_valid_processed[X_full_train.columns]\n",
        "valid_predictions = final_model.predict(df_valid_processed)\n",
        "df_submission = pd.DataFrame({'SalesID': df_valid_raw['SalesID'], 'SalePrice': valid_predictions})\n",
        "submission_filename = f'{DRIVE_PATH}submission.csv'\n",
        "df_submission.to_csv(submission_filename, index=False)\n",
        "print(f\"\\nקובץ ההגשה '{submission_filename}' נשמר בהצלחה ב-Google Drive!\")\n",
        "print(df_submission.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZGqq23GKSRF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPqoX3W2Kgqf",
        "outputId": "a1d0bf65-a3ab-4816-d24c-0513ed1c8eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- מתחיל חיפוש אקראי של היפר-פרמטרים... (זה עשוי לקחת זמן) ---\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   4.7s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   4.4s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.2s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   4.8s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.6s\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time=   6.6s\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time=   8.7s\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time=   6.6s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  31.1s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  29.9s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  29.1s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  23.6s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  25.6s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  24.6s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.4s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.3s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.3s\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.3s\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   4.8s\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   5.0s\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  12.0s\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  14.8s\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  12.9s\n",
            "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=4, min_samples_split=5, n_estimators=150; total time= 1.2min\n",
            "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=4, min_samples_split=5, n_estimators=150; total time= 1.3min\n",
            "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=4, min_samples_split=5, n_estimators=150; total time= 1.3min\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
            "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time= 1.2min\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time= 1.3min\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time= 1.2min\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time= 1.4min\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time= 1.5min\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=150; total time= 1.5min\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   8.5s\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   9.6s\n",
            "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  10.0s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   6.1s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   8.1s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   6.2s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  26.1s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  25.1s\n",
            "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  23.8s\n",
            "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=  18.7s\n",
            "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=  18.9s\n",
            "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=  20.1s\n",
            "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=150; total time=  16.6s\n",
            "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=150; total time=  17.1s\n",
            "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=150; total time=  16.7s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=150; total time=  10.7s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=150; total time=  11.1s\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=150; total time=  10.0s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  50.5s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  49.2s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  49.9s\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time= 1.5min\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time= 1.5min\n",
            "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time= 1.6min\n",
            "\n",
            "הפרמטרים הטובים ביותר שנמצאו בחיפוש האקראי:\n",
            "{'n_estimators': 150, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 0.5, 'max_depth': 30}\n",
            "\n",
            "ערך הפיספוס (RMSLE) עם המודל המכוונן הוא: 0.24463\n",
            "\n",
            "==================================================\n",
            "--- שלב סופי: אימון מחדש על כל נתוני האימון עם הפרמטרים המיטביים ---\n",
            "אימון המודל הסופי והמכוונן הושלם.\n"
          ]
        }
      ],
      "source": [
        "# 6.3: יצירה והרצה של החיפוש האקראי\n",
        "print(\"\\n--- מתחיל חיפוש אקראי של היפר-פרמטרים... (זה עשוי לקחת זמן) ---\")\n",
        "\n",
        "# ... (הגדרת param_dist ו-rs נשארת זהה)\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_features': [0.5, 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "rs_base_model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=rs_base_model,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=20,\n",
        "                                   cv=3,\n",
        "                                   verbose=2,\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=1)\n",
        "\n",
        "# התיקון כאן: הרצת החיפוש על האובייקט הנכון\n",
        "random_search.fit(X_train_time, y_train_time)\n",
        "\n",
        "\n",
        "# 6.4: הצגת הפרמטרים הטובים ביותר שנמצאו\n",
        "print(\"\\nהפרמטרים הטובים ביותר שנמצאו בחיפוש האקראי:\")\n",
        "# שימוש באובייקט הנכון כדי לקבל את התוצאות\n",
        "best_model_random = random_search.best_estimator_\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# 6.5: הערכת המודל עם הפרמטרים הטובים ביותר\n",
        "val_preds = best_model_random.predict(X_val_time)\n",
        "val_rmsle_score = rmsle(y_val_time, val_preds)\n",
        "print(f\"\\nערך הפיספוס (RMSLE) עם המודל המכוונן הוא: {val_rmsle_score:.5f}\")\n",
        "\n",
        "# --- שלב 7: אימון מודל סופי עם הפרמטרים הטובים ביותר ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- שלב סופי: אימון מחדש על כל נתוני האימון עם הפרמטרים המיטביים ---\")\n",
        "X_full_train = df_train_processed.drop('SalePrice', axis=1)\n",
        "y_full_train = df_train_processed['SalePrice']\n",
        "\n",
        "# התיקון כאן: שימוש בפרמטרים מהאובייקט הנכון\n",
        "final_model = RandomForestRegressor(n_jobs=-1,\n",
        "                                    random_state=42,\n",
        "                                    **random_search.best_params_)\n",
        "\n",
        "final_model.fit(X_full_train, y_full_train)\n",
        "print(\"אימון המודל הסופי והמכוונן הושלם.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'n_estimators': 150, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 0.5, 'max_depth': 30}\n"
      ],
      "metadata": {
        "id": "22EaixgRQMF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- שלב 8: יצירת קובץ הגשה על נתוני 2012 ---\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n--- יצירת קובץ הגשה על נתוני 2012 באמצעות המודל הסופי ---\")\n",
        "train_cols = set(X_full_train.columns)\n",
        "valid_cols = set(df_valid_processed.columns)\n",
        "missing_in_valid = list(train_cols - valid_cols)\n",
        "for c in missing_in_valid:\n",
        "    df_valid_processed[c] = 0\n",
        "df_valid_processed = df_valid_processed[X_full_train.columns]\n",
        "valid_predictions = final_model.predict(df_valid_processed)\n",
        "df_submission = pd.DataFrame({'SalesID': df_valid_raw['SalesID'], 'SalePrice': valid_predictions})\n",
        "submission_filename = f'{DRIVE_PATH}submission.csv'\n",
        "df_submission.to_csv(submission_filename, index=False)\n",
        "print(f\"\\nקובץ ההגשה '{submission_filename}' נשמר בהצלחה ב-Google Drive!\")\n",
        "print(df_submission.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMbuYNN5Q1Nb",
        "outputId": "6c41e36b-11f7-4b2e-a05b-9a5bb62707da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "--- יצירת קובץ הגשה על נתוני 2012 באמצעות המודל הסופי ---\n",
            "\n",
            "קובץ ההגשה '/content/drive/MyDrive/KaggleProject/submission.csv' נשמר בהצלחה ב-Google Drive!\n",
            "   SalesID     SalePrice\n",
            "0  1222837  45631.943917\n",
            "1  1222839  71520.674511\n",
            "2  1222841  34326.652104\n",
            "3  1222843  16260.024516\n",
            "4  1222845  40855.074722\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu3V7SlRAJUum3X/hs/jdP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}